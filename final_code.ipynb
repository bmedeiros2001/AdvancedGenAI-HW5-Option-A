{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe9fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add this cell BEFORE starting servers\n",
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# # Kill any existing processes on our ports\n",
    "# for port in [10020, 10021, 10022, 8001]:\n",
    "#     try:\n",
    "#         # Find and kill process using the port (macOS/Linux)\n",
    "#         result = subprocess.run(\n",
    "#             f\"lsof -ti:{port} | xargs kill -9\",\n",
    "#             shell=True,\n",
    "#             capture_output=True\n",
    "#         )\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "# print(\"Cleared ports 10020, 10021, 10022, 8001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4c31ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: a2a-sdk\n",
      "Version: 0.3.20\n",
      "Name: google-adk\n",
      "Version: 1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip show a2a-sdk google-adk | grep -E \"^(Name|Version)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d992c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from a2a.client import client as real_client_module\n",
    "from a2a.client.card_resolver import A2ACardResolver\n",
    "\n",
    "\n",
    "class PatchedClientModule:\n",
    "    def __init__(self, real_module) -> None:\n",
    "        for attr in dir(real_module):\n",
    "            if not attr.startswith('_'):\n",
    "                setattr(self, attr, getattr(real_module, attr))\n",
    "        self.A2ACardResolver = A2ACardResolver\n",
    "\n",
    "\n",
    "patched_module = PatchedClientModule(real_client_module)\n",
    "sys.modules['a2a.client.client'] = patched_module  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6482bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from typing import Any\n",
    "import uuid\n",
    "\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "from a2a.client import ClientConfig, ClientFactory, create_text_message_object\n",
    "from a2a.types import (\n",
    "    AgentCapabilities,\n",
    "    AgentCard,\n",
    "    AgentSkill,\n",
    "    TransportProtocol,\n",
    "    Message, Part, TextPart, Role\n",
    ")\n",
    "from a2a.utils.constants import AGENT_CARD_WELL_KNOWN_PATH\n",
    "from dotenv import load_dotenv\n",
    "from google.adk.a2a.utils.agent_to_a2a import to_a2a\n",
    "from google.adk.agents import Agent, SequentialAgent\n",
    "from google.adk.agents.remote_a2a_agent import RemoteA2aAgent\n",
    "\n",
    "from google.adk.tools import google_search\n",
    "from google.adk.tools.mcp_tool.mcp_toolset import MCPToolset\n",
    "from google.adk.tools.mcp_tool.mcp_session_manager import SseServerParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0891a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "MCP_SERVER_URL = \"http://127.0.0.1:8001/sse\"\n",
    "\n",
    "CUSTOMER_DATA_AGENT_URL = \"http://127.0.0.1:10020\"\n",
    "SUPPORT_AGENT_URL = \"http://127.0.0.1:10021\"\n",
    "ROUTER_AGENT_URL = \"http://127.0.0.1:10022\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31373794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"google.adk\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"a2a\").setLevel(logging.WARNING)\n",
    "\n",
    "logger = logging.getLogger(\"customer_service_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c50e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import API key\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed0020",
   "metadata": {},
   "source": [
    "# Defining Agents\n",
    "\n",
    "1. Router Agent\n",
    "2. Customer Data Agent\n",
    "3. Support Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3598cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Support Agent\n",
    "def build_support_agent() -> Agent:\n",
    "    mcp_toolset = MCPToolset(\n",
    "        connection_params=SseServerParams(\n",
    "            url=MCP_SERVER_URL,\n",
    "            timeout=300.0,\n",
    "            sse_read_timeout=300.0\n",
    "        )\n",
    "    )\n",
    "    instruction = \"\"\"\n",
    "    You are the Support Agent. You handle customer support requests.\n",
    "    \n",
    "    Available MCP Tools:\n",
    "    - create_ticket(customer_id, issue, priority): Create a support ticket\n",
    "    - get_customer_history(customer_id): Get ticket history\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Analyze customer issues and determine priority (low/medium/high)\n",
    "    - Create tickets for issues that need tracking\n",
    "    - Provide helpful responses to customer queries\n",
    "    - Escalate urgent issues (billing disputes, account access) as high priority\n",
    "    \n",
    "    When you receive customer context from another agent, use it to personalize your response.\n",
    "    \n",
    "    Priority Guidelines:\n",
    "    - HIGH: Billing issues, account access problems, security concerns\n",
    "    - MEDIUM: Feature questions, general complaints, update requests\n",
    "    - LOW: General inquiries, feature requests\n",
    "    \n",
    "    return Agent(\n",
    "        model=MODEL_NAME,\n",
    "        name=\"router_agent\",\n",
    "        instruction=instruction,\n",
    "        tools=[mcp_toolset]\n",
    "    )\n",
    "    \"\"\"\n",
    "    return Agent(\n",
    "        model=MODEL_NAME,\n",
    "        name=\"support_agent\",\n",
    "        instruction=instruction\n",
    "    )\n",
    "\n",
    "# Customer data\n",
    "def build_customer_data_agent() -> Agent:\n",
    "    mcp_toolset = MCPToolset(\n",
    "        connection_params=SseServerParams(\n",
    "            url=MCP_SERVER_URL,\n",
    "            timeout=300.0,\n",
    "            sse_read_timeout=300.0\n",
    "        )\n",
    "    )\n",
    "    instruction = \"\"\"\n",
    "    You are the Customer Data Agent. You have exclusive access to the customer database.\n",
    "    \n",
    "    Available MCP Tools:\n",
    "    - get_customer(customer_id): Get a single customer by ID\n",
    "    - list_customers(status, limit): List customers by status ('active' or 'disabled')\n",
    "    - update_customer(customer_id, data): Update customer fields\n",
    "    - create_ticket(customer_id, issue, priority): Create a support ticket\n",
    "    - get_customer_history(customer_id): Get all tickets for a customer\n",
    "    \n",
    "    When asked for customer information:\n",
    "    1. Use the appropriate MCP tool\n",
    "    2. Return the data in a clear, structured format\n",
    "    3. Include all relevant fields\n",
    "    \n",
    "    Always respond with factual data from the database. Never make up customer information.\n",
    "    \"\"\"\n",
    "    return Agent(\n",
    "        model=MODEL_NAME,\n",
    "        name=\"customer_data_agent\",\n",
    "        instruction=instruction,\n",
    "        tools=[mcp_toolset],\n",
    "    )\n",
    "\n",
    "# Router\n",
    "def build_router_agent(remote_customer_data: RemoteA2aAgent, remote_support: RemoteA2aAgent) -> Agent:\n",
    "    instruction = \"\"\"\n",
    "    You are the Support Agent. You handle customer support requests.\n",
    "    \n",
    "    Available MCP Tools:\n",
    "    - create_ticket(customer_id, issue, priority): Create a support ticket\n",
    "    - get_customer_history(customer_id): Get all tickets for a customer\n",
    "    \n",
    "    IMPORTANT: You can call get_customer_history multiple times for different customers.\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Analyze customer issues and determine priority (low/medium/high)\n",
    "    - Create tickets for issues that need tracking\n",
    "    - Provide helpful responses to customer queries\n",
    "    - Escalate urgent issues (billing disputes, account access) as high priority\n",
    "    \n",
    "    Priority Guidelines:\n",
    "    - HIGH: Billing issues, account access problems, security concerns\n",
    "    - MEDIUM: Feature questions, general complaints, update requests\n",
    "    - LOW: General inquiries, feature requests\n",
    "\"\"\"\n",
    "    host = SequentialAgent(\n",
    "        name=\"router_host_agent\",\n",
    "        sub_agents=[remote_customer_data, remote_support]\n",
    "    )\n",
    "    return host\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80129b0",
   "metadata": {},
   "source": [
    "# A2A Orchestration Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a318c8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2A server infrastructure defined (using to_a2a).\n"
     ]
    }
   ],
   "source": [
    "# Simplified Server Infrastructure using to_a2a\n",
    "async def run_agent_server(agent: Any, agent_card: AgentCard, port: int) -> None:\n",
    "    print(f\"Starting server for {agent.name} on port {port}...\")\n",
    "    # to_a2a converts the agent to a Starlette app automatically\n",
    "    app = to_a2a(agent, agent_card=agent_card, port=port)\n",
    "    \n",
    "    config = uvicorn.Config(\n",
    "        app,\n",
    "\n",
    "        port=port,\n",
    "        log_level=\"warning\",\n",
    "        loop=\"none\",\n",
    "    )\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()\n",
    "\n",
    "print(\"A2A server infrastructure defined (using to_a2a).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aefe279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AgentCard fields:\n",
      "  additional_interfaces: optional\n",
      "  capabilities: REQUIRED\n",
      "  default_input_modes: REQUIRED\n",
      "  default_output_modes: REQUIRED\n",
      "  description: REQUIRED\n",
      "  documentation_url: optional\n",
      "  icon_url: optional\n",
      "  name: REQUIRED\n",
      "  preferred_transport: optional\n",
      "  protocol_version: optional\n",
      "  provider: optional\n",
      "  security: optional\n",
      "  security_schemes: optional\n",
      "  signatures: optional\n",
      "  skills: REQUIRED\n",
      "  supports_authenticated_extended_card: optional\n",
      "  url: REQUIRED\n",
      "  version: REQUIRED\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAgentCard fields:\")\n",
    "for field_name, field_info in AgentCard.model_fields.items():\n",
    "    required = \"REQUIRED\" if field_info.is_required() else \"optional\"\n",
    "    print(f\"  {field_name}: {required}\")\n",
    "from google.adk.a2a.utils.agent_to_a2a import to_a2a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da1054b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Cards defined:\n",
      "  - Customer Data Agent\n",
      "  - Support Agent\n",
      "  - Router Agent\n"
     ]
    }
   ],
   "source": [
    "# Customer Data Agent Card\n",
    "customer_data_agent_card = AgentCard(\n",
    "    name=\"Customer Data Agent\",\n",
    "    url=CUSTOMER_DATA_AGENT_URL,\n",
    "    description=\"Accesses customer and ticket data via MCP tools. The only agent with direct database access.\",\n",
    "    version=\"1.0\",\n",
    "    capabilities=AgentCapabilities(streaming=True),\n",
    "    defaultInputModes=[\"text/plain\"],\n",
    "    defaultOutputModes=[\"application/json\"],\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"customer_data_operations\",\n",
    "            name=\"Customer Data Operations\",\n",
    "            description=\"Retrieve, update, and manage customer records and ticket history\",\n",
    "            tags=[\"customer\", \"database\", \"mcp\", \"tickets\"],\n",
    "            examples=[\n",
    "                \"Get customer information for ID 5\",\n",
    "                \"List all active customers\",\n",
    "                \"Update customer email\",\n",
    "                \"Show ticket history for customer 3\",\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Support Agent Card\n",
    "support_agent_card = AgentCard(\n",
    "    name=\"Support Agent\",\n",
    "    url=SUPPORT_AGENT_URL,\n",
    "    description=\"Handles customer support conversations, assesses priorities, and creates tickets\",\n",
    "    version=\"1.0\",\n",
    "    capabilities=AgentCapabilities(streaming=True),\n",
    "    defaultInputModes=[\"text/plain\"],\n",
    "    defaultOutputModes=[\"application/json\"],\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"support_handling\",\n",
    "            name=\"Customer Support Handling\",\n",
    "            description=\"Handle support issues, assess priority, create tickets, provide resolutions\",\n",
    "            tags=[\"support\", \"billing\", \"escalation\", \"tickets\"],\n",
    "            examples=[\n",
    "                \"I need help with my account\",\n",
    "                \"I was charged twice, please refund\",\n",
    "                \"How do I upgrade my subscription?\",\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Router Agent Card\n",
    "router_agent_card = AgentCard(\n",
    "    name=\"Router Agent\",\n",
    "    url=ROUTER_AGENT_URL,\n",
    "    description=\"Orchestrates customer queries by coordinating between Customer Data and Support agents\",\n",
    "    version=\"1.0\",\n",
    "    capabilities=AgentCapabilities(streaming=True),\n",
    "    defaultInputModes=[\"text/plain\"],\n",
    "    defaultOutputModes=[\"application/json\"],\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"query_orchestration\",\n",
    "            name=\"Customer Service Orchestration\",\n",
    "            description=\"Routes queries to appropriate agents and coordinates multi-agent responses\",\n",
    "            tags=[\"routing\", \"orchestration\", \"multi-agent\"],\n",
    "            examples=[\n",
    "                \"I'm customer 5 and need help upgrading my account\",\n",
    "                \"I've been charged twice, please refund immediately!\",\n",
    "                \"Update my email and show my ticket history\",\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Agent Cards defined:\")\n",
    "print(f\"  - {customer_data_agent_card.name}\")\n",
    "print(f\"  - {support_agent_card.name}\")\n",
    "print(f\"  - {router_agent_card.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bdb2e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built specialist agents:\n",
      "  - customer_data_agent\n",
      "  - support_agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qf/l8sfrcpd6vq2564v1_qt31fm0000gn/T/ipykernel_63428/1112252722.py:45: DeprecationWarning: MCPToolset class is deprecated, use `McpToolset` instead.\n",
      "  mcp_toolset = MCPToolset(\n",
      "/var/folders/qf/l8sfrcpd6vq2564v1_qt31fm0000gn/T/ipykernel_63428/1112252722.py:3: DeprecationWarning: MCPToolset class is deprecated, use `McpToolset` instead.\n",
      "  mcp_toolset = MCPToolset(\n"
     ]
    }
   ],
   "source": [
    "# Build the actual ADK agents\n",
    "customer_data_agent = build_customer_data_agent()\n",
    "support_agent = build_support_agent()\n",
    "\n",
    "print(\"Built specialist agents:\")\n",
    "print(f\"  - {customer_data_agent.name}\")\n",
    "print(f\"  - {support_agent.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9020ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created remote agent wrappers:\n",
      "  - customer_data_remote -> http://127.0.0.1:10020\n",
      "  - support_remote -> http://127.0.0.1:10021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qf/l8sfrcpd6vq2564v1_qt31fm0000gn/T/ipykernel_63428/176174927.py:2: UserWarning: [EXPERIMENTAL] RemoteA2aAgent: ADK Implementation for A2A support (A2aAgentExecutor, RemoteA2aAgent and corresponding supporting components etc.) is in experimental mode and is subjected to breaking changes. A2A protocol and SDK arethemselves not experimental. Once it's stable enough the experimental mode will be removed. Your feedback is welcome.\n",
      "  remote_customer_data_agent = RemoteA2aAgent(\n",
      "/var/folders/qf/l8sfrcpd6vq2564v1_qt31fm0000gn/T/ipykernel_63428/176174927.py:8: UserWarning: [EXPERIMENTAL] RemoteA2aAgent: ADK Implementation for A2A support (A2aAgentExecutor, RemoteA2aAgent and corresponding supporting components etc.) is in experimental mode and is subjected to breaking changes. A2A protocol and SDK arethemselves not experimental. Once it's stable enough the experimental mode will be removed. Your feedback is welcome.\n",
      "  remote_support_agent = RemoteA2aAgent(\n"
     ]
    }
   ],
   "source": [
    "# Create remote agents\n",
    "remote_customer_data_agent = RemoteA2aAgent(\n",
    "    name=\"customer_data_remote\",\n",
    "    description=\"Remote A2A wrapper for Customer Data Agent - accesses database via MCP\",\n",
    "    agent_card=f\"{CUSTOMER_DATA_AGENT_URL}{AGENT_CARD_WELL_KNOWN_PATH}\",\n",
    ")\n",
    "\n",
    "remote_support_agent = RemoteA2aAgent(\n",
    "    name=\"support_remote\",\n",
    "    description=\"Remote A2A wrapper for Support Agent - handles customer issues\",\n",
    "    agent_card=f\"{SUPPORT_AGENT_URL}{AGENT_CARD_WELL_KNOWN_PATH}\",\n",
    ")\n",
    "\n",
    "print(\"Created remote agent wrappers:\")\n",
    "print(f\"  - {remote_customer_data_agent.name} -> {CUSTOMER_DATA_AGENT_URL}\")\n",
    "print(f\"  - {remote_support_agent.name} -> {SUPPORT_AGENT_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21df810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built router agent: router_host_agent\n",
      "  Sub-agents: ['customer_data_remote', 'support_remote']\n"
     ]
    }
   ],
   "source": [
    "# Router uses the remote wrappers to coordinate\n",
    "router_agent = build_router_agent(\n",
    "    remote_customer_data=remote_customer_data_agent,\n",
    "    remote_support=remote_support_agent,\n",
    ")\n",
    "\n",
    "print(f\"Built router agent: {router_agent.name}\")\n",
    "print(f\"  Sub-agents: {[a.name for a in router_agent.sub_agents]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7853f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server startup functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Start A2A Server\n",
    "async def start_all_servers() -> None:\n",
    "    tasks = [\n",
    "        asyncio.create_task(\n",
    "            run_agent_server(customer_data_agent, customer_data_agent_card, 10020)\n",
    "        ),\n",
    "        asyncio.create_task(\n",
    "            run_agent_server(support_agent, support_agent_card, 10021)\n",
    "        ),\n",
    "        asyncio.create_task(\n",
    "            run_agent_server(router_agent, router_agent_card, 10022)\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # Wait a moment for servers to start\n",
    "    await asyncio.sleep(2.0)\n",
    "    \n",
    "    logger.info(\"A2A servers started:\")\n",
    "    logger.info(f\"  Customer Data Agent: {CUSTOMER_DATA_AGENT_URL}\")\n",
    "    logger.info(f\"  Support Agent:       {SUPPORT_AGENT_URL}\")\n",
    "    logger.info(f\"  Router Agent:        {ROUTER_AGENT_URL}\")\n",
    "    \n",
    "    try:\n",
    "        await asyncio.gather(*tasks)\n",
    "    except asyncio.CancelledError:\n",
    "        logger.info(\"Server tasks cancelled\")\n",
    "\n",
    "\n",
    "def run_servers_in_background() -> None:\n",
    "    \"\"\"\n",
    "    Run servers in a background thread (for notebook compatibility).\n",
    "    \"\"\"\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    try:\n",
    "        loop.run_until_complete(start_all_servers())\n",
    "    except Exception as e:\n",
    "        print(f\"Server error: {e}\")\n",
    "\n",
    "print(\"Server startup functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35b6190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2a.types import Message, Part, TextPart, Role\n",
    "from google.adk.a2a.utils.agent_to_a2a import to_a2a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b2e88d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2A Client defined (using raw HTTP).\n"
     ]
    }
   ],
   "source": [
    "# A2A CLIENT - For sending queries to agents\n",
    "import uuid\n",
    "from uuid import uuid4\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from a2a.types import SendMessageRequest, MessageSendParams\n",
    "from a2a.client import A2AClient as A2ABaseClient\n",
    "from a2a.client.card_resolver import A2ACardResolver\n",
    "\n",
    "class A2AClient:\n",
    "    \"\"\"\n",
    "    Simple client for calling A2A servers.\n",
    "    Used to send test queries to the Router Agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, default_timeout: float = 240.0):\n",
    "        self._agent_card_cache: Dict[str, Any] = {}\n",
    "        self.default_timeout = default_timeout\n",
    "    \n",
    "    async def send_query(self, agent_url: str, message: str) -> str:\n",
    "        \"\"\"\n",
    "        Send a query to an A2A agent and get the response.\n",
    "        \n",
    "        Args:\n",
    "            agent_url: Base URL of the agent (e.g., http://127.0.0.1:10022)\n",
    "            message: The query to send\n",
    "            \n",
    "        Returns:\n",
    "            The agent's response text\n",
    "        \"\"\"\n",
    "        timeout_config = httpx.Timeout(\n",
    "            timeout=self.default_timeout,\n",
    "            connect=10.0,\n",
    "            read=self.default_timeout,\n",
    "            write=10.0,\n",
    "            pool=5.0,\n",
    "        )\n",
    "        \n",
    "        async with httpx.AsyncClient(timeout=timeout_config) as client:\n",
    "            jsonrpc_request = {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": uuid4().hex,\n",
    "                \"method\": \"message/send\",\n",
    "                \"params\": {\n",
    "                    \"message\": {\n",
    "                        \"role\": \"user\",\n",
    "                        \"parts\": [\n",
    "                            {\"kind\": \"text\", \"text\": message}\n",
    "                        ],\n",
    "                        \"messageId\": uuid4().hex  # camelCase - THIS IS THE KEY!\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            \n",
    "\n",
    "            # Send and get response (non-streaming)\n",
    "            response = await client.post(\n",
    "                agent_url,\n",
    "                json=jsonrpc_request,\n",
    "                headers={\"Content-Type\": \"application/json\"}\n",
    "            )\n",
    "            \n",
    "            result = response.json()\n",
    "            \n",
    "            if \"error\" in result:\n",
    "                return f\"Error: {result['error']}\"\n",
    "            \n",
    "            # Extract text from successful response\n",
    "            if \"result\" in result:\n",
    "                res = result[\"result\"]\n",
    "                \n",
    "                # Handle Message response\n",
    "                if \"parts\" in res:\n",
    "                    for part in res[\"parts\"]:\n",
    "                        if \"text\" in part:\n",
    "                            return part[\"text\"]\n",
    "                \n",
    "                # Handle Task response with artifacts\n",
    "                if \"artifacts\" in res:\n",
    "                    for artifact in res[\"artifacts\"]:\n",
    "                        if \"parts\" in artifact:\n",
    "                            for part in artifact[\"parts\"]:\n",
    "                                if \"text\" in part:\n",
    "                                    return part[\"text\"]\n",
    "                \n",
    "                # Handle Task response - return status info\n",
    "                if \"status\" in res:\n",
    "                    status = res[\"status\"]\n",
    "                    if \"message\" in status:\n",
    "                        return status[\"message\"]\n",
    "                    return f\"Task status: {status.get('state', 'unknown')}\"\n",
    "                \n",
    "                return json.dumps(res, indent=2)\n",
    "            \n",
    "            return str(result)\n",
    "\n",
    "print(\"A2A Client defined (using raw HTTP).\")\n",
    "from google.adk.a2a.utils.agent_to_a2a import to_a2a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2c6b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_test_scenarios() -> None:\n",
    "    client = A2AClient()\n",
    "    \n",
    "    # Scenario 1\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 1: Simple Query\")\n",
    "    print(\"=\"*70)\n",
    "    response1 = await client.send_query(ROUTER_AGENT_URL, \"Get customer information for ID 5\")\n",
    "    print(f\"\\nRESPONSE:\\n{response1}\")\n",
    "    \n",
    "    print(\"\\n⏳ Waiting 15 seconds...\")\n",
    "    await asyncio.sleep(15)\n",
    "    \n",
    "    # Scenario 2\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 2: Coordinated Query\")\n",
    "    print(\"=\"*70)\n",
    "    response2 = await client.send_query(ROUTER_AGENT_URL, \"I'm customer 5 and need help upgrading my account to premium tier.\")\n",
    "    print(f\"\\nRESPONSE:\\n{response2}\")\n",
    "    \n",
    "    print(\"\\n⏳ Waiting 15 seconds...\")\n",
    "    await asyncio.sleep(15)\n",
    "    \n",
    "    # Scenario 3\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 3: Complex Query\")\n",
    "    print(\"=\"*70)\n",
    "    response3 = await client.send_query(ROUTER_AGENT_URL, \"Show me all active customers who have open tickets. Summarize them grouped by customer with ticket priorities.\")\n",
    "    print(f\"\\nRESPONSE:\\n{response3}\")\n",
    "    \n",
    "    print(\"\\n⏳ Waiting 15 seconds...\")\n",
    "    await asyncio.sleep(15)\n",
    "    \n",
    "    # Scenario 4\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 4: Escalation\")\n",
    "    print(\"=\"*70)\n",
    "    response4 = await client.send_query(ROUTER_AGENT_URL, \"I'm customer 1. I've been charged twice, please refund immediately! I am very upset.\")\n",
    "    print(f\"\\nRESPONSE:\\n{response4}\")\n",
    "    \n",
    "    print(\"\\n⏳ Waiting 15 seconds...\")\n",
    "    await asyncio.sleep(15)\n",
    "    \n",
    "    # Scenario 5\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 5: Multi-Intent\")\n",
    "    print(\"=\"*70)\n",
    "    response5 = await client.send_query(ROUTER_AGENT_URL, \"I'm customer 2. Update my email to new@email.com and then show my ticket history.\")\n",
    "    print(f\"\\nRESPONSE:\\n{response5}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ALL TEST SCENARIOS COMPLETED\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737dfe60",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5998330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qf/l8sfrcpd6vq2564v1_qt31fm0000gn/T/ipykernel_63428/1105539043.py:5: UserWarning: [EXPERIMENTAL] to_a2a: ADK Implementation for A2A support (A2aAgentExecutor, RemoteA2aAgent and corresponding supporting components etc.) is in experimental mode and is subjected to breaking changes. A2A protocol and SDK arethemselves not experimental. Once it's stable enough the experimental mode will be removed. Your feedback is welcome.\n",
      "  app = to_a2a(agent, agent_card=agent_card, port=port)\n",
      "/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/google/adk/a2a/utils/agent_to_a2a.py:137: UserWarning: [EXPERIMENTAL] A2aAgentExecutor: ADK Implementation for A2A support (A2aAgentExecutor, RemoteA2aAgent and corresponding supporting components etc.) is in experimental mode and is subjected to breaking changes. A2A protocol and SDK arethemselves not experimental. Once it's stable enough the experimental mode will be removed. Your feedback is welcome.\n",
      "  agent_executor = A2aAgentExecutor(\n",
      "/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/google/adk/a2a/executor/a2a_agent_executor.py:101: UserWarning: [EXPERIMENTAL] A2aAgentExecutorConfig: ADK Implementation for A2A support (A2aAgentExecutor, RemoteA2aAgent and corresponding supporting components etc.) is in experimental mode and is subjected to breaking changes. A2A protocol and SDK arethemselves not experimental. Once it's stable enough the experimental mode will be removed. Your feedback is welcome.\n",
      "  self._config = config or A2aAgentExecutorConfig()\n",
      "/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/google/adk/a2a/utils/agent_to_a2a.py:149: UserWarning: [EXPERIMENTAL] AgentCardBuilder: ADK Implementation for A2A support (A2aAgentExecutor, RemoteA2aAgent and corresponding supporting components etc.) is in experimental mode and is subjected to breaking changes. A2A protocol and SDK arethemselves not experimental. Once it's stable enough the experimental mode will be removed. Your feedback is welcome.\n",
      "  card_builder = AgentCardBuilder(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] Starting A2A servers in background...\n",
      "[2/3] Waiting for servers to initialize...\n",
      "Starting server for customer_data_agent on port 10020...\n",
      "Starting server for support_agent on port 10021...\n",
      "Starting server for router_host_agent on port 10022...\n",
      "[3/3] Running test scenarios...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "    \n",
    "print(\"\\n[1/3] Starting A2A servers in background...\")\n",
    "server_thread = threading.Thread(\n",
    "    target=run_servers_in_background,\n",
    "    daemon=True,\n",
    ")\n",
    "server_thread.start()\n",
    "\n",
    "print(\"[2/3] Waiting for servers to initialize...\")\n",
    "time.sleep(5.0)\n",
    "\n",
    "    \n",
    "print(\"[3/3] Running test scenarios...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "645b5f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:39:10,818 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCENARIO 1: Simple Query\n",
      "Query: 'Get customer information for ID 5'\n",
      "Expected: Single agent, straightforward MCP call\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:39:12,392 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:12,421 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:13,273 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:13,302 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:14,404 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE:\n",
      "Here is the information for customer ID 5:\n",
      "\n",
      "*   **ID**: 5\n",
      "*   **Name**: Eve Martinez\n",
      "*   **Email**: eve@email.com\n",
      "*   **Phone**: 555-0105\n",
      "*   **Status**: active\n",
      "*   **Created At**: 2025-12-04 19:55:31\n",
      "*   **Updated At**: 2025-12-04 19:55:31\n"
     ]
    }
   ],
   "source": [
    "async def run_test_scenarios() -> None:\n",
    "    client = A2AClient()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Scenario 1: Simple Query\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 1: Simple Query\")\n",
    "    print(\"Query: 'Get customer information for ID 5'\")\n",
    "    print(\"Expected: Single agent, straightforward MCP call\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    response1 = await client.send_query(\n",
    "        ROUTER_AGENT_URL,\n",
    "        \"Get customer information for ID 5\"\n",
    "    )\n",
    "    print(f\"\\nRESPONSE:\\n{response1}\")\n",
    "\n",
    "await run_test_scenarios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b60b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:39:14,491 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCENARIO 2: Coordinated Query\n",
      "Query: 'I'm customer 5 and need help upgrading my account'\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:39:16,228 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:16,304 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:18,165 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:18,216 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:24,000 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:24,019 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:26,347 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE:\n",
      "Thank you for letting me know! I see that a support ticket (ID: 33) has already been created for you to upgrade your account to the premium tier. A representative will be in touch with you shortly to assist with the upgrade.\n"
     ]
    }
   ],
   "source": [
    "async def run_test_scenarios() -> None:\n",
    "    client = A2AClient()\n",
    "    # =========================================================================\n",
    "    # Scenario 2: Coordinated Query\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 2: Coordinated Query\")\n",
    "    print(\"Query: 'I'm customer 5 and need help upgrading my account'\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    response2 = await client.send_query(\n",
    "        ROUTER_AGENT_URL,\n",
    "        \"I'm customer 5 and need help upgrading my account to premium tier.\"\n",
    "    )\n",
    "    print(f\"\\nRESPONSE:\\n{response2}\")\n",
    "await run_test_scenarios()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a6087e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:39:26,427 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCENARIO 3: Complex Query\n",
      "Query: 'Show me all active customers who have open tickets'\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:39:29,478 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:29,541 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:32,706 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:32,764 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:38,750 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:38,779 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:40,717 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE:\n",
      "Here is a summary of all active customers with open tickets, grouped by customer and their ticket priorities:\n",
      "\n",
      "**Alice Johnson (ID: 1)**\n",
      "*   **Open Ticket Priorities**: 6 High, 1 Medium\n",
      "\n",
      "**Charlie Brown (ID: 3)**\n",
      "*   **Open Ticket Priorities**: 2 High\n",
      "\n",
      "**Eve Martinez (ID: 5)**\n",
      "*   **Open Ticket Priorities**: 2 High, 4 Medium\n",
      "\n",
      "**Frank Wilson (ID: 6)**\n",
      "*   **Open Ticket Priorities**: 1 High\n",
      "\n",
      "**Grace Lee (ID: 7)**\n",
      "*   **Open Ticket Priorities**: 1 Medium\n",
      "\n",
      "**Ivy Chen (ID: 9)**\n",
      "*   **Open Ticket Priorities**: 1 Low\n",
      "\n",
      "**Jack Thompson (ID: 10)**\n",
      "*   **Open Ticket Priorities**: 1 Low\n",
      "\n",
      "**Karen White (ID: 11)**\n",
      "*   **Open Ticket Priorities**: 2 High\n",
      "\n",
      "**Maria Rodriguez (ID: 13)**\n",
      "*   **Open Ticket Priorities**: 1 Medium\n",
      "\n",
      "**Nathan Kim (ID: 14)**\n",
      "*   **Open Ticket Priorities**: 1 Low\n",
      "\n",
      "**Olivia Taylor (ID: 15)**\n",
      "*   **Open Ticket Priorities**: 1 High\n"
     ]
    }
   ],
   "source": [
    "async def run_test_scenarios() -> None:\n",
    "    client = A2AClient()\n",
    "    # =========================================================================\n",
    "    # Scenario 3: Complex Query\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 3: Complex Query\")\n",
    "    print(\"Query: 'Show me all active customers who have open tickets'\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    response3 = await client.send_query(\n",
    "        ROUTER_AGENT_URL,\n",
    "        \"Show me all active customers who have open tickets. Summarize them grouped by customer with ticket priorities.\"\n",
    "    )\n",
    "    print(f\"\\nRESPONSE:\\n{response3}\")\n",
    "await run_test_scenarios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fed99ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:39:40,762 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCENARIO 4: Escalation\n",
      "Query: 'I've been charged twice, please refund immediately!'\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:39:41,930 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:41,962 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:43,902 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:43,923 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:46,032 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE:\n",
      "Hello! I understand how frustrating it must be to be charged twice, and I apologize for the inconvenience this has caused.\n",
      "\n",
      "I see that a high-priority ticket (ID 34) has already been created for you regarding the duplicate charge and immediate refund request. Our team is actively investigating this issue and will work to resolve it promptly. You will be contacted as soon as there is an update.\n"
     ]
    }
   ],
   "source": [
    "async def run_test_scenarios() -> None:\n",
    "    client = A2AClient()\n",
    "    # =========================================================================\n",
    "    # Scenario 4: Escalation\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 4: Escalation\")\n",
    "    print(\"Query: 'I've been charged twice, please refund immediately!'\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    response4 = await client.send_query(\n",
    "        ROUTER_AGENT_URL,\n",
    "        \"I'm customer 1. I've been charged twice, please refund immediately! I am very upset.\"\n",
    "    )\n",
    "    print(f\"\\nRESPONSE:\\n{response4}\")\n",
    "await run_test_scenarios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fff46f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:39:46,081 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCENARIO 5: Multi-Intent\n",
      "Query: 'Update my email to new@email.com and show my ticket history'\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:39:46,907 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:46,939 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:47,447 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:47,491 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:48,605 - INFO - Response received from the model.\n",
      "2025-12-04 22:39:48,635 - INFO - Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-12-04 22:39:50,471 - INFO - Response received from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE:\n",
      "Your email has been updated to new@email.com. Here is your ticket history:\n",
      "\n",
      "*   **Ticket ID:** 4\n",
      "    *   **Customer ID:** 2\n",
      "    *   **Issue:** Need help upgrading subscription\n",
      "    *   **Status:** in_progress\n",
      "    *   **Priority:** low\n",
      "    *   **Created At:** 2025-12-04 19:55:31\n",
      "\n",
      "*   **Ticket ID:** 5\n",
      "    *   **Customer ID:** 2\n",
      "    *   **Issue:** Billing cycle question\n",
      "    *   **Status:** resolved\n",
      "    *   **Priority:** low\n",
      "    *   **Created At:** 2025-12-04 19:55:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:52:48,173 - ERROR - Error in sse_reader\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpx/_transports/default.py\", line 271, in __aiter__\n",
      "    async for part in self._httpcore_stream:\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 407, in __aiter__\n",
      "    raise exc from None\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 403, in __aiter__\n",
      "    async for part in self._stream:\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 342, in __aiter__\n",
      "    raise exc\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 334, in __aiter__\n",
      "    async for chunk in self._connection._receive_response_body(**kwargs):\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 203, in _receive_response_body\n",
      "    event = await self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 213, in _receive_event\n",
      "    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/mcp/client/sse.py\", line 81, in sse_reader\n",
      "    async for sse in event_source.aiter_sse():  # pragma: no branch\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpx_sse/_api.py\", line 42, in aiter_sse\n",
      "    async for line in lines:\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpx_sse/_api.py\", line 80, in _aiter_sse_lines\n",
      "    async for text in response.aiter_text():\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpx/_models.py\", line 1018, in aiter_text\n",
      "    async for byte_content in self.aiter_bytes():\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpx/_models.py\", line 997, in aiter_bytes\n",
      "    async for raw_bytes in self.aiter_raw():\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpx/_models.py\", line 1055, in aiter_raw\n",
      "    async for raw_stream_bytes in self.stream:\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpx/_client.py\", line 176, in __aiter__\n",
      "    async for chunk in self._stream:\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpx/_transports/default.py\", line 270, in __aiter__\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/Users/brunamedeiros/Documents/GitHub/AdvancedGenAI-HW5-Option-A/.venv/lib/python3.11/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n"
     ]
    }
   ],
   "source": [
    "async def run_test_scenarios() -> None:\n",
    "    client = A2AClient()\n",
    "    # =========================================================================\n",
    "    # Scenario 5: Multi-Intent\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCENARIO 5: Multi-Intent\")\n",
    "    print(\"Query: 'Update my email to new@email.com and show my ticket history'\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    response5 = await client.send_query(\n",
    "        ROUTER_AGENT_URL,\n",
    "        \"I'm customer 2. Update my email to new@email.com and then show my ticket history.\"\n",
    "    )\n",
    "    print(f\"\\nRESPONSE:\\n{response5}\")\n",
    "await run_test_scenarios()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5976fdd1",
   "metadata": {},
   "source": [
    "**Main challenges:** I had a few challenges during this assignment. \n",
    "- I first tried building the system with LangGraph, and I truly had a tough time. I thought the SDK system was indeed easier to set up. Nonetheless, it was still challenging. I struggled to workout the two different ways A2A could've been set up. Originally, I attempted to do manual set up, but later on I changed to `to_a2a()`.\n",
    "- Additionally, I spent way too long debugging why my `AgentCards` weren't working. While the python module uses snake_case, the `AgentCard` parameters need parameters in camelCase.\n",
    "- I would constantly getthis cryptic error: \"Expected response header Content-Type to contain 'text/event-stream', got 'application/json'\". My A2A client was trying to stream responses but the server was sending regular JSON. The agents were speaking different protocols. Had to make sure everyone was on the same page about how they're communicating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a44f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HW5 (venv)",
   "language": "python",
   "name": "hw5-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
